{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tugas \u00b6 Tugas Penambangan dan Pencarian Web AL ABRAR HILMI UBAIDAH 160411100148","title":"Home"},{"location":"#tugas","text":"Tugas Penambangan dan Pencarian Web AL ABRAR HILMI UBAIDAH 160411100148","title":"Tugas"},{"location":"webmining/","text":"Web Mining \u00b6 Web mining atau data mining web adalah proses menemukan hubungan intrinsic (misalkan, informasi yang menarik dan bermanfaat) dari data web, yang disajikan dalam bentuk teks, link, atau informasi penggunaan. Istilah web mining pertama kali digunakaan oleh Etzioni pada tahun 1996 (pakar data mining); saat ini, banyak konferensi, jurnal, dan buku berfokus pada data mining web. Ini memang area teknologi dan praktik bisnis yang berevolusi secara berkelanjutan. Dalam kesempatan kali ini, sya akan mencoba menjelaskan bagaimana cara crawling data yang dimana ata yang akan kita ambil berupa text dari salah satu website, lalu data itu saya akan lakukan proses Text Preprocessing , lalu berlanjut ke proses Seleksi Fitur , dan yang terakhir akan dilakukan proses Clustering. untuk seluruh file anda bisa ambil Pendahuluan \u00b6 Nama : AL ABRAR HILMI UBAIDAH NIM : 160411100148 Mata Kuliah : Penambangan dan Pencarian Web Jurusan : Teknik Informatika Universitas Trunojoyo Madura Dosen Pengampu : Mulaab, S.Si., M.Kom. Yang Harus Disiapkan \u00b6 Sebelum memulai semua proses harap menyiapkan semua aplikasi dan library yang dibutuhkan. Berikut adalah list aplikasi dan library yang akan digunakan: Bahasa Pemrogaman Python 2.7 Web yang akan dituju. Disini saya akan mengambil data dari web : mainbasket.com Library Python dan tambahan data seperti : Library BeautifulSoup4 Library xlsxwriter Library xlrd Library pandas Library numpy Library scikit learn Library Sastrawi #NB: Selain menggunakan aplikasi python 2.7 kalian juga dapat menggunakan aplikasi spyder , menggunakan spyder lebih memudahkan anda karena di aplikasi tersebut sudah terinstall semua library yang kita butuhkan di project kali ini. Data Kamu besar bahasa Indonesia Tambahan Stopword **#NB : Ada beberapa progam yang memerlukan koneksi internet. Crawling \u00b6 Apa itu Crawling? \u00b6 Crawlimg adalah aplikasi script program untuk melakukan scan kesemua halamandi internet dan dibuatkan index untuk data yang di carinya. Nama lain untuk web adalah web spider, web robot, bot, crawl dan automatic indexer. Search engine menggunakan web crawl untuk mengumpulkan informasi mengenai apa yang ada di halaman-halaman web publik. Tujuan utamanya adalah mengumpukan data sehingga ketika pengguna Internet mengetikkan kata pencarian di komputernya, search engine dapat dengan segera menampilkan web site yang relevan. Web crawl bisa beroperasi hanya sekali, misalnya untuk suatu projek yang hanya sekali jalan, atau jika tujuannya untuk jangka panjang seperti pada kasus search engine, mereka bisa diprogram untuk menyisir Internet secara periodik untuk menentukan apakah sudah berlangsung perubahan signifikan. Jika suatu situs mengalami trafik sangat padat atau kesulitan teknis, spider atau crawl dapat diprogram untuk mencatat hal ini dan mengunjunginya kembali setelah kesulitan teknis itu terselesaikan. Langkah - Langkah \u00b6 Ada beberapa langkah yang harus dilalui untuk melakukan crawling data, langkah pertama yaitu : MengCraw Link \u00b6 Mengcraw link yang dimaksud disini adalah mengambil seluruh link yang ada didalam satu page website, lalu link berita tersebut selanjutnya disimpan dalam file berbentuk excel. File mainbasket_url.py mainbasket_url.py berfungsi untuk mengambil link berita pada setiap page url header, proses pertama ketika melakukan crawling pada sebuah website berita adalah melakukan pengambilan setiap link berita pada header berita. import xlsxwriter workbook = xlsxwriter.Workbook('mainbasket_daftarlinkberita.xlsx') worksheet1 = workbook.add_worksheet() kumpulan_link=[] code tersebut berfungsi untuk membuat file excel yang nantinya file excel tersebut akan menyimpan semua link yang telah di crawling. def cari_link(page): import urllib as link from bs4 import BeautifulSoup import re print (page) html_page = link.urlopen(\"https://www.mainbasket.com/c/4/berita/ablpage=\"+str(page)).read() soup = BeautifulSoup(html_page, \"html.parser\") Code tersebut merupakan proses crawling yang kemudian dikonversi ke dalam tag lxml(html) , pada website https://www.mainbasket.com/c/4/berita/ablpage=\"+str(page) , proses crawling tersebut menggunakan library beautifulsoup4 for a in soup.findAll('div', 'post-title'): #print a for link in a.findAll('a', attrs={'href': re.compile(\"^https://\")}): print link.get('href') kumpulan_link.append(link.get('href')) Code diatas digunakan untuk mengambil value data url berita pada tag html yang mengandung attribut \"href\" pada setiap page halaman website, dimana attribut \"href\" tersebut terletak di dalam tag \"a\" dengan class \"post-title\". NB untuk mendapatkan tag filter html, diperlukan melakukan inspect element pada web browser untuk mengatahui tata letak attribut tag dan class nya. for i in range(1,6): cari_link(str(i)) print len(kumpulan_link) for baris in range(len(kumpulan_link)): print (baris) print (kumpulan_link[baris]) worksheet1.write(baris,1, str(kumpulan_link[baris])) workbook.close() Code tersebut untuk menjalankan crawling data,dimana di setiap page terdapat 7 link atau berita, disini saya batasi sampai 5 page saja. jadi total keseluruhan link yang saya ambil sebanyak 35 link atau berita, kemudian hasil crawling tersebut terciptaah sebuah file excel baru yang berisi isi list data link seperti gambar berikut. Crawl Judul dan Isi Berita \u00b6 Proses yang harus dilakukan se;anjutnya yaitu mengambil data berupa text judul dan isi berita dari setiap link berita. File mainbasket_kedb.py File ini digunakan untuk melakukan crawling text judul dan isi berita pada setiap link di file excel link berita , lalu menyimpannya pada database. for baris in range(data.nrows): print (\">>>>>>>>>>>\"+str(baris)) from bs4 import BeautifulSoup value_data = data.cell_value(rowx=baris, colx=1) print (value_data) html = link.urlopen(str(value_data)).read() soup = BeautifulSoup(html, \"html.parser\") Code diatas digunakan untuk mengunjungi setiap link header berita yang tersimpan pada file excel, untuk selanjutnya dilakukan proses crawl data text judul dan isi berita dari masing-masing link berita yang sudah ada. soup = BeautifulSoup(html, \"html.parser\") berita = soup.find('div', 'headline-title').get_text() berita = berita.strip() print(berita) isi = soup.find('div', 'post-body').get_text() isi=isi.replace('var unruly = window.unruly || {};unruly.native = unruly.native |{};unruly.native.siteId = 1082418;', '').replace('\\t\\t', '').replace(',', '').replace('googletag.cmd.push(function() { googletag.display(\"div-Inside-MediumRectangle\"); });','') isi=isi.replace(\"googletag.cmd.push(function() { googletag.display('div-Inside-MediumRectangle'); });\",'') isi=isi.replace(\"\\n\",'') isi= isi[22:] isi=isi.strip() Code tersebut berfungsi untuk melakukan crawling data text judul dan isi berita pada setiap link berita, untuk melakukan pengambilan judul dilakukan pada tag html ('div', 'headline-title'), dan isi berita pada tag html ('div', 'post-body') pada masing-masing link yang telah di ambil sebelumnya. Berikut hasil output nya pada file excel Text Prepocessing \u00b6 Definisi Text Preprocessing adalah suatu proses pengubahan bentuk data yang belum terstruktur menjadi data yang terstruktur sesuai dengan kebutuhan, untuk proses mining yang lebih lanjut (sentiment analysis, peringkasan, clustering dokumen, etc.). Pada intinya Text Preprocessing adalah merubah teks menjadi term index yang bertujuan menghasilkan sebuah set term index yang bisa mewakili dokumen. Proses Text Prepocessing dapat menggunakan library Sastrawi Langkah - Langkah \u00b6 Tokenisasi \u00b6 Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token-token/bagian-bagian tertentu. Sebagai contoh, tokenisasi dari kalimat \"Aku baru saja makan bakso pedas\" menghasilkan enam token, yakni: \"Aku\", \"baru\", \"saja\", \"makan\", \"bakso\", \"pedas\". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Stopword Removal \u00b6 Stop word merupakan kata yang diabaikan dalam pemrosesan, kata-kata ini biasanya disimpan ke dalam stop lists. Karakteristik utama dalam pemilihan stop word biasanya adalah kata yang mempunyai frekuensi kemunculan yang tinggi misalnya kata penghubung seperti \u201cdan\u201d, \u201catau\u201d, \u201ctapi\u201d, \u201cakan\u201d dan lainnya. Tidak ada aturan pasti dalam menentukan stop word yang akan digunakan, penentuan stop word bisa disesuaikan dengan kasus yang sedang diselesaikan. Tujuan utama dalam penerapan proses Stopword Removal adalah mengurangi jumlah kata dalam sebuah dokumen yang nantinya akan berpengaruh dalam kecepatan dan peforma. mainbasket_sastrawi.py File diatas berfungsi untuk melakukan proses prepocessing seperti pengecekan setiap kata dari hasil proses tokenisasi pada database bahasa indonesia serta pengaplikasian stopword . factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() stop = stopword.remove(kalimat) stop = re.sub(r'\\b\\w{1,2}\\b', '', stop) Code tersebut berfungsi untuk melakukan proses menghilangkan kata kata hubung pada dokumen. Stemming \u00b6 Stemming merupakan suatu proses untuk menemukan kata dasar dari sebuah kata. Dengan menghilangkan semua imbuhan baik yang terdiri dari awalan , sisipan , akhiran dan confixes pada kata turunan. Stemming digunakan untuk mengganti bentuk dari suatu kata menjadi kata dasar dari kata tersebut yang sesuai dengan struktur morfologi Bahasa Indonesia yang baik dan benar. factory = StemmerFactory() stemmer = factory.create_stemmer() katadasar = stemmer.stem(str(kalimat)) katadasar = re.sub(r'\\b\\w{1,3}\\b', '', katadasar).replace('-','') Masih pada file yang sama code tersebut berfungsi untuk melakukan penghilangan pada setiap kata yang dianggap sebagai kata ber-imbuhan pada data menggunakan library dari sastrawi. mainbasket_cekkamus.py File mainbasket_cekkamus berfungi untuk melakukan pengecekan kata dengan database kata KBBI. Hasil \u00b6 Setelah di run maka akan terbuat file baru berbentuk excel yang akan menampung hasil Text Prepocessing, berikut hasil file excel tersebut. Vector Space Model \u00b6 Vector Space Model (VSM) adalah suatu model yang digunakan untuk mengukur kemiripan antara suatu dokumen dengan suatu query . Query dan dokumen dianggap sebagai vektor-vektor pada ruang n-dimensi, dimana t adalah jumlah dari seluruh term yang ada dalam leksikon. Leksikon adalah daftar semua term yang ada dalam indeks. Menggunakan Metode \u00b6 Bag-of-Word \u00b6 Sebuah konsep yang diambil dari analisis teks, yaitu merepresentasikan dokumen sebagai sebuah kantung informasi-informasi penting tanpa mengurutkan setiap katanya. mainbasket_ambilfitur.py File mainbasket_ambilfitur.py ini berfungsi untuk mengambilseluruh kata pada semua dokumen. mainbasket_sastrawi_seringmuncul.py File mainbasket_sastrawi_seringmuncul.py ini digunakan untuk menghitung frekuensi kemunculan setiap kata dalam bentuk matrik kemudian hasil tersebut akan disimpan dalam bentuk file excel baru seperti pada gambar dibawah ini. TF-IDF \u00b6 TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. Sedangkan IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. Rumus dari TF-IDF adalah TF x IDF. mainbasket_tfidf.py File mainbasket_tfidf.py ini berfungsi untuk melakukan proses perhitungan nilai setiap kata IDF terlebih dahulu, lalu akan melakukan perhitungan nilai TF-IDF dan menjadikan ke bentuk matrik VSM. for i in range(len(list_fitur)): temp=list_fitur[i] data[temp]*=w[i] print (data.head()) data.to_csv('baru_tfidf.csv') code tersebut adalah sebuah rumus untuk menghitung TF-IDF lalu hasil perhitungan tersebut diletakkan dalam file berformat CSV. Berikut adalah hasil dari file CSV. Seleksi Fitur \u00b6 Definisi \u00b6 Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi. Metode \u00b6 Dalam seleksi fitur kali ini, saya menggunakan beberapa metode , berikut adalah metode yang saya gunakan : Principal Component Analysis Model Based Ranking Univariate Feature Selection Langsuh saja kita bahas satu persatu 1. Principal Component Analysis \u00b6 PCA (Principal Component Analysis) adalah teknik yang digunakan untuk menyederhanakan suatu data, dengan cara mentransformasi data secara linier sehingga terbentuk sistem koordinat baru dengan varians maksimum. Analisis komponen utama dapat digunakan untuk mereduksi dimensi suatu data tanpa mengurangi karakteristik data tersebut secara signifikan. Analisis komponen utama juga sering digunakan untuk menghindari masalah multikolinearitas antar peubah bebas dalam model regresi berganda. 2. Model Based Ranking \u00b6 Model Based Ranking disini memungkinkan untuk memberi peringkat. Modl Based Ranking ini menggunakan prosedur Random Forest . Random Forest \u00b6 Random forest (RF) adalah suatu algoritma yang digunakan pada klasifikasi data dalam jumlah yang besar. Klasifikasi random forest dilakukan melalui penggabungan pohon (tree) dengan melakukan training pada sampel data yang dimiliki. Penggunaan pohon (tree) yang semakin banyak akan mempengaruhi akurasi yang akan didapatkan menjadi lebih baik. Penentuan klasifikasi dengan random forest diambil berdasarkan hasil voting dari tree yang terbentuk. Pemenang dari tree yang terbentuk ditentukan dengan vote terbanyak. Pembangunan pohon (tree) pada random forest sampai dengan mencapai ukuran maksimum dari pohon data. def RandomForest(): from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score clf = RandomForestClassifier(n_estimators=2, max_depth=4) scores = [] num_features = len(X.columns) for i in range(num_features): col = X.columns[i] score = np.mean(cross_val_score(clf, X[col].values.reshape(-1, 1), y, cv=7)) scores.append((int(score * 100), col)) print (\">>>>>>>>>>>>>>>>>>>>\") print (i, \">\", col, \"score:\", score) Code tersebut berfungsi untuk melakukan proses perankingan semua fitur menggunakan Random Forest yang bertujuan untuk melihat fitur yang paling berpengaruh. scores = sorted(scores, reverse=True) print (\"Random Forest>>>>>>>\") print(\"The 5 best features selected by this method are :\") for i in range(5): print(scores[i][1]) print (\"The 5 worst features selected by this method are :\") for i in range(5): print(scores[len(scores) - 1 - i][1]) Code tersebut berfungsi untuk menampilkan 5 fitur dengan ranking terbaik dan 5 fitur dengan ranking terendah. 3. Univariate Feature Selection \u00b6 Univariate Feature Selection disini untuk memilih fitur terbaik .Univariate Feature Selection ini menggunakan prosedur Chi Square. Chi Square \u00b6 Prosedur Chi Square ini dengan mentabulasikan suatu variabel menjadi kategori dan menghitung statistik chi square. Uji kecocokan modelnya membandingkan observasi dan frekuensi harapan pada kategori untuk diuji tiap kategorinya. Uji Chi Square digunakan untuk menguji hubungan atau pengaruh dua buah variabel nominal dan mengukur kuatnya hubungan antar variabel. def UFS(cek, n_ranking): from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2, mutual_info_classif if cek == 1: print (\"UFS (Univariate Feature Selection) model chi^2 >>>>>>>\") test = SelectKBest(score_func=chi2, k=2) elif cek == 2: print (\"UFS (Univariate Feature Selection) mutual_info_classif >>>>>>>\") test = SelectKBest(score_func=mutual_info_classif, k=2) test.fit(X, y) scores = [] for i in range(num_features): score = test.scores_[i] scores.append((score, X.columns[i])) print (sorted(scores, reverse=True)) print_best_worst(scores, n_ranking) Code tersebut digunakan untuk menjalankan proses prosedur chi square . Clustering \u00b6 Definisi \u00b6 Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Metode Clustering sendiri ada banyak.Kali ini yang saya gunakan adalah K-Means Clustering dan Shilhouette Coefisient. K-means \u00b6 K-means merupakan salah satu algoritma clustering. Tujuan algoritma ini yaitu untuk membagi data menjadi beberapa kelompok. Algoritma ini menerima masukan berupa data tanpa label kelas. Hal ini berbeda dengan supervised learning yang menerima masukan berupa vektor (\u00adx\u00ad1 , y1) , (\u00adx\u00ad2 , y2) , \u2026, (\u00adx\u00adi , yi), di mana xi merupakan data dari suatu data pelatihan dan yi merupakan label kelas untuk xi. Algoritma ini akan mengelompokkan data atau objek ke dalam k buah kelompok tersebut. Pada setiap cluster terdapat titik pusat (centroid) yang merepresentasikan cluster tersebut. Algoritma K-Means : Tentukan jumlah klaster yang ingin dibentuk dan tetapkan pusat cluster k. Menggunakan jarak euclidean kemudian hitung setiap data ke pusat cluster. Kelompokkan data ke dalam cluster dengan jarak yang paling pendek dengan persamaan. Hitung pusat cluster yang baru menggunakan persamaan. Ulangi langkah dua sampai dengan empat sehingga sudah tidak ada lagi data yang berpindah ke kluster yang lain. python clusterer = KMeans(n_clusters=n_cluster) preds = clusterer.fit_predict((df)) centers = clusterer.cluster_centers_ Code diatas menyatakan banyaknya cluster. Shilhouette Coefisient \u00b6 Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. Tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a i Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b i . Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : Clustering K-mean dengan total corpus (total fitur) asli. \u00b6 banyak_cluster = list(range(2, 150)) for n_cluster in banyak_cluster: clusterer = KMeans(n_clusters=n_cluster) preds = clusterer.fit_predict((df)) centers = clusterer.cluster_centers_ score = silhouette_score(df, preds, metric='euclidean') temp.append(score) temp_pred.append(preds) # print (\"Untuk kluster={},silhoute score :{} \".format(n_cluster, repr(score))) # print(preds) print (\"kluster terbaik\") print (\"kluster ke > \" + str(temp.index(max(temp)) + 2) + \" >silhout> \" + str(max(temp))) Code tersebut berfungsi untuk melakukan proses clustering menggunakan K-mean clustering dengan menggunakan euclidean serta menghitung nilai silhoutte pada setiap cluster dengan menggunakan total corpus asli, untuk selanjutnya menampilkan nilai silhoutte terbaik pada cluster tertentu. Clustering (K-mean) + Feature Selection (Random Forest). \u00b6 def RFE(cek, n_ranking): from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression if cek == 1: print (\"RFE (Recursive Feature Elimination) model LogisticRegression >>>>>>>\") rfe = RFE(LogisticRegression(), n_features_to_select=1) elif cek == 2: print (\"RFE (Recursive Feature Elimination) model RandomForest >>>>>>>\") rfe = RFE(RandomForestClassifier(n_estimators=10), n_features_to_select=1) rfe.fit(X, y) scores = [] for i in range(num_features): scores.append((rfe.ranking_[i], X.columns[i])) print (sorted(scores, reverse=True)) print_best_worst(scores, n_ranking) Code tersebut berfungsi untuk melakukan proses seleksi fitur menggunakan metode Random Forest menggunakan n_estimator pohon keputusan. def UFS(cek, n_ranking): from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2, mutual_info_classif if cek == 1: print (\"UFS (Univariate Feature Selection) model chi^2 >>>>>>>\") test = SelectKBest(score_func=chi2, k=2) elif cek == 2: print (\"UFS (Univariate Feature Selection) mutual_info_classif >>>>>>>\") test = SelectKBest(score_func=mutual_info_classif, k=2) test.fit(X, y) scores = [] for i in range(num_features): score = test.scores_[i] scores.append((score, X.columns[i])) print (sorted(scores, reverse=True)) print_best_worst(scores, n_ranking) Code diatas digunakan untuk melakukan proses seleksi fitur menggunakan metode Chi Square . Evaluasi \u00b6 Dalam kasus ini data yang digunakan oleh penulis berasal dari website \"https://www.mainbasket.com/c/4/berita/abl\" dengan page sebanyak 5 page yang dimana setiap page terdapat 7 berita, total keseluruhan ada 35 data , yang terdiri dari judul serta isi berita. Hasil \u00b6 Seleksi Fitur Random Forest dengan K-mean \u00b6 Pada percobaan ini, penulis menngunakan seleksi fitur random forest namun dengan nilai n yang berbeda. Perlu diketahui bahwa n menunjukkan banyaknya pohon keputusan yang terdapat pada random forest. Percobaan n_estimator Banyak Fitur yang digunakan Silhoutte terbaik n_Cluster ke 1 5 100 0.5403243593327233 2 ke 2 5 50 0.2160825606285164 5 ke 3 50 100 0.516949295993263 2 ke 4 50 50 0.24651372752710796 3 Hasil Percobaan Terbaik \u00b6 Seleksi Fitur Chi Square dengan K-mean \u00b6 Pada percobaan ini, penulis menngunakan seleksi fitur chi square namun dengan menggunakan banyak fitur yang digunakan secara berbeda. Percobaan Banyak Fitur yang digunakan Silhoutte terbaik n_Cluster ke 1 35 0.6134604631137835 2 ke 2 100 0.5327485060311361 2 Hasil Percobaan ke 1 Hasil Percobaan ke 2 Kesimpulan \u00b6 Dari beberapa percobaan diatas, dapat menyimpulkan bahwa metode clustering K-mean dengan seleksi fitur \"Random Forest\" menggunakan jumlah fitur sebanyak 100 fitur serta menggunkan n_estimator=5 adalah metode terbaik sejauh ini. Refrensi : https://yudiagusta.wordpress.com/clustering/ https://id.wikipedia.org/wiki/K-means http://depandienda.it.student.pens.ac.id/file/kmeans_references.pdf https://www.researchgate.net/publication/323365687_Algoritme_Genetika_Untuk_Optimasi_K-Means_Clustering_Dalam_Pengelompokan_Data_Tsunami https://repository.ipb.ac.id/handle/123456789/14020 https://id.wikipedia.org/wiki/Analisis_komponen_utama https://id.wikipedia.org/wiki/Random_forest http://datariset.com/olahdata/detail/olah-data-jogja-uji-chi-square https://www.kaggle.com/dkim1992/feature-selection-ranking https://www.datariset.com/olahdata/detail/olah-data-jogja-uji-chi-square https://www.rumusstatistik.com/2015/03/analisis-komponen-utama-principal.html https://liyantanto.wordpress.com/2011/06/28/pencarian-dengan-metode-vektor-space-model-vsm/ https://repository.widyatama.ac.id/xmlui/bitstream/handle/123456789/7151/Daftar%20Istilah.pdf?sequence=8 https://informatikalogi.com/term-weighting-tf-idf/ https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&ved=2ahUKEwjS4bfq6_rhAhW763MBHdFRCvgQFjAEegQIARAC&url=http%3A%2F%2Fmalifauzi.lecture.ub.ac.id%2Ffiles%2F2016%2F02%2FText-Pre-Processing-v2.pptx&usg=AOvVaw3lqK3cdfxwBPYWPH1omJ-w https://id.wikipedia.org/wiki/Tokenisasi https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/ https://devtrik.com/python/text-preprocessing-dengan-python-nltk/ https://informatikalogi.com/text-preprocessing/ https://www.kompasiana.com/jalaludin.ax/54f67c7fa333112b758b4ebf/membuat-web-crawling http://prowebscraping.com/web-scraping-vs-web-crawling/ https://beritati.blogspot.com/2015/06/sekilas-tentang-web-mining-penambangan.html","title":"Web Content Mining"},{"location":"webmining/#web-mining","text":"Web mining atau data mining web adalah proses menemukan hubungan intrinsic (misalkan, informasi yang menarik dan bermanfaat) dari data web, yang disajikan dalam bentuk teks, link, atau informasi penggunaan. Istilah web mining pertama kali digunakaan oleh Etzioni pada tahun 1996 (pakar data mining); saat ini, banyak konferensi, jurnal, dan buku berfokus pada data mining web. Ini memang area teknologi dan praktik bisnis yang berevolusi secara berkelanjutan. Dalam kesempatan kali ini, sya akan mencoba menjelaskan bagaimana cara crawling data yang dimana ata yang akan kita ambil berupa text dari salah satu website, lalu data itu saya akan lakukan proses Text Preprocessing , lalu berlanjut ke proses Seleksi Fitur , dan yang terakhir akan dilakukan proses Clustering. untuk seluruh file anda bisa ambil","title":"Web Mining"},{"location":"webmining/#pendahuluan","text":"Nama : AL ABRAR HILMI UBAIDAH NIM : 160411100148 Mata Kuliah : Penambangan dan Pencarian Web Jurusan : Teknik Informatika Universitas Trunojoyo Madura Dosen Pengampu : Mulaab, S.Si., M.Kom.","title":"Pendahuluan"},{"location":"webmining/#yang-harus-disiapkan","text":"Sebelum memulai semua proses harap menyiapkan semua aplikasi dan library yang dibutuhkan. Berikut adalah list aplikasi dan library yang akan digunakan: Bahasa Pemrogaman Python 2.7 Web yang akan dituju. Disini saya akan mengambil data dari web : mainbasket.com Library Python dan tambahan data seperti : Library BeautifulSoup4 Library xlsxwriter Library xlrd Library pandas Library numpy Library scikit learn Library Sastrawi #NB: Selain menggunakan aplikasi python 2.7 kalian juga dapat menggunakan aplikasi spyder , menggunakan spyder lebih memudahkan anda karena di aplikasi tersebut sudah terinstall semua library yang kita butuhkan di project kali ini. Data Kamu besar bahasa Indonesia Tambahan Stopword **#NB : Ada beberapa progam yang memerlukan koneksi internet.","title":"Yang Harus Disiapkan"},{"location":"webmining/#crawling","text":"","title":"Crawling"},{"location":"webmining/#apa-itu-crawling","text":"Crawlimg adalah aplikasi script program untuk melakukan scan kesemua halamandi internet dan dibuatkan index untuk data yang di carinya. Nama lain untuk web adalah web spider, web robot, bot, crawl dan automatic indexer. Search engine menggunakan web crawl untuk mengumpulkan informasi mengenai apa yang ada di halaman-halaman web publik. Tujuan utamanya adalah mengumpukan data sehingga ketika pengguna Internet mengetikkan kata pencarian di komputernya, search engine dapat dengan segera menampilkan web site yang relevan. Web crawl bisa beroperasi hanya sekali, misalnya untuk suatu projek yang hanya sekali jalan, atau jika tujuannya untuk jangka panjang seperti pada kasus search engine, mereka bisa diprogram untuk menyisir Internet secara periodik untuk menentukan apakah sudah berlangsung perubahan signifikan. Jika suatu situs mengalami trafik sangat padat atau kesulitan teknis, spider atau crawl dapat diprogram untuk mencatat hal ini dan mengunjunginya kembali setelah kesulitan teknis itu terselesaikan.","title":"Apa itu Crawling?"},{"location":"webmining/#langkah-langkah","text":"Ada beberapa langkah yang harus dilalui untuk melakukan crawling data, langkah pertama yaitu :","title":"Langkah - Langkah"},{"location":"webmining/#mengcraw-link","text":"Mengcraw link yang dimaksud disini adalah mengambil seluruh link yang ada didalam satu page website, lalu link berita tersebut selanjutnya disimpan dalam file berbentuk excel. File mainbasket_url.py mainbasket_url.py berfungsi untuk mengambil link berita pada setiap page url header, proses pertama ketika melakukan crawling pada sebuah website berita adalah melakukan pengambilan setiap link berita pada header berita. import xlsxwriter workbook = xlsxwriter.Workbook('mainbasket_daftarlinkberita.xlsx') worksheet1 = workbook.add_worksheet() kumpulan_link=[] code tersebut berfungsi untuk membuat file excel yang nantinya file excel tersebut akan menyimpan semua link yang telah di crawling. def cari_link(page): import urllib as link from bs4 import BeautifulSoup import re print (page) html_page = link.urlopen(\"https://www.mainbasket.com/c/4/berita/ablpage=\"+str(page)).read() soup = BeautifulSoup(html_page, \"html.parser\") Code tersebut merupakan proses crawling yang kemudian dikonversi ke dalam tag lxml(html) , pada website https://www.mainbasket.com/c/4/berita/ablpage=\"+str(page) , proses crawling tersebut menggunakan library beautifulsoup4 for a in soup.findAll('div', 'post-title'): #print a for link in a.findAll('a', attrs={'href': re.compile(\"^https://\")}): print link.get('href') kumpulan_link.append(link.get('href')) Code diatas digunakan untuk mengambil value data url berita pada tag html yang mengandung attribut \"href\" pada setiap page halaman website, dimana attribut \"href\" tersebut terletak di dalam tag \"a\" dengan class \"post-title\". NB untuk mendapatkan tag filter html, diperlukan melakukan inspect element pada web browser untuk mengatahui tata letak attribut tag dan class nya. for i in range(1,6): cari_link(str(i)) print len(kumpulan_link) for baris in range(len(kumpulan_link)): print (baris) print (kumpulan_link[baris]) worksheet1.write(baris,1, str(kumpulan_link[baris])) workbook.close() Code tersebut untuk menjalankan crawling data,dimana di setiap page terdapat 7 link atau berita, disini saya batasi sampai 5 page saja. jadi total keseluruhan link yang saya ambil sebanyak 35 link atau berita, kemudian hasil crawling tersebut terciptaah sebuah file excel baru yang berisi isi list data link seperti gambar berikut.","title":"MengCraw Link"},{"location":"webmining/#crawl-judul-dan-isi-berita","text":"Proses yang harus dilakukan se;anjutnya yaitu mengambil data berupa text judul dan isi berita dari setiap link berita. File mainbasket_kedb.py File ini digunakan untuk melakukan crawling text judul dan isi berita pada setiap link di file excel link berita , lalu menyimpannya pada database. for baris in range(data.nrows): print (\">>>>>>>>>>>\"+str(baris)) from bs4 import BeautifulSoup value_data = data.cell_value(rowx=baris, colx=1) print (value_data) html = link.urlopen(str(value_data)).read() soup = BeautifulSoup(html, \"html.parser\") Code diatas digunakan untuk mengunjungi setiap link header berita yang tersimpan pada file excel, untuk selanjutnya dilakukan proses crawl data text judul dan isi berita dari masing-masing link berita yang sudah ada. soup = BeautifulSoup(html, \"html.parser\") berita = soup.find('div', 'headline-title').get_text() berita = berita.strip() print(berita) isi = soup.find('div', 'post-body').get_text() isi=isi.replace('var unruly = window.unruly || {};unruly.native = unruly.native |{};unruly.native.siteId = 1082418;', '').replace('\\t\\t', '').replace(',', '').replace('googletag.cmd.push(function() { googletag.display(\"div-Inside-MediumRectangle\"); });','') isi=isi.replace(\"googletag.cmd.push(function() { googletag.display('div-Inside-MediumRectangle'); });\",'') isi=isi.replace(\"\\n\",'') isi= isi[22:] isi=isi.strip() Code tersebut berfungsi untuk melakukan crawling data text judul dan isi berita pada setiap link berita, untuk melakukan pengambilan judul dilakukan pada tag html ('div', 'headline-title'), dan isi berita pada tag html ('div', 'post-body') pada masing-masing link yang telah di ambil sebelumnya. Berikut hasil output nya pada file excel","title":"Crawl Judul dan Isi Berita"},{"location":"webmining/#text-prepocessing","text":"Definisi Text Preprocessing adalah suatu proses pengubahan bentuk data yang belum terstruktur menjadi data yang terstruktur sesuai dengan kebutuhan, untuk proses mining yang lebih lanjut (sentiment analysis, peringkasan, clustering dokumen, etc.). Pada intinya Text Preprocessing adalah merubah teks menjadi term index yang bertujuan menghasilkan sebuah set term index yang bisa mewakili dokumen. Proses Text Prepocessing dapat menggunakan library Sastrawi","title":"Text Prepocessing"},{"location":"webmining/#langkah-langkah_1","text":"","title":"Langkah - Langkah"},{"location":"webmining/#tokenisasi","text":"Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token-token/bagian-bagian tertentu. Sebagai contoh, tokenisasi dari kalimat \"Aku baru saja makan bakso pedas\" menghasilkan enam token, yakni: \"Aku\", \"baru\", \"saja\", \"makan\", \"bakso\", \"pedas\". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca.","title":"Tokenisasi"},{"location":"webmining/#stopword-removal","text":"Stop word merupakan kata yang diabaikan dalam pemrosesan, kata-kata ini biasanya disimpan ke dalam stop lists. Karakteristik utama dalam pemilihan stop word biasanya adalah kata yang mempunyai frekuensi kemunculan yang tinggi misalnya kata penghubung seperti \u201cdan\u201d, \u201catau\u201d, \u201ctapi\u201d, \u201cakan\u201d dan lainnya. Tidak ada aturan pasti dalam menentukan stop word yang akan digunakan, penentuan stop word bisa disesuaikan dengan kasus yang sedang diselesaikan. Tujuan utama dalam penerapan proses Stopword Removal adalah mengurangi jumlah kata dalam sebuah dokumen yang nantinya akan berpengaruh dalam kecepatan dan peforma. mainbasket_sastrawi.py File diatas berfungsi untuk melakukan proses prepocessing seperti pengecekan setiap kata dari hasil proses tokenisasi pada database bahasa indonesia serta pengaplikasian stopword . factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() stop = stopword.remove(kalimat) stop = re.sub(r'\\b\\w{1,2}\\b', '', stop) Code tersebut berfungsi untuk melakukan proses menghilangkan kata kata hubung pada dokumen.","title":"Stopword Removal"},{"location":"webmining/#stemming","text":"Stemming merupakan suatu proses untuk menemukan kata dasar dari sebuah kata. Dengan menghilangkan semua imbuhan baik yang terdiri dari awalan , sisipan , akhiran dan confixes pada kata turunan. Stemming digunakan untuk mengganti bentuk dari suatu kata menjadi kata dasar dari kata tersebut yang sesuai dengan struktur morfologi Bahasa Indonesia yang baik dan benar. factory = StemmerFactory() stemmer = factory.create_stemmer() katadasar = stemmer.stem(str(kalimat)) katadasar = re.sub(r'\\b\\w{1,3}\\b', '', katadasar).replace('-','') Masih pada file yang sama code tersebut berfungsi untuk melakukan penghilangan pada setiap kata yang dianggap sebagai kata ber-imbuhan pada data menggunakan library dari sastrawi. mainbasket_cekkamus.py File mainbasket_cekkamus berfungi untuk melakukan pengecekan kata dengan database kata KBBI.","title":"Stemming"},{"location":"webmining/#hasil","text":"Setelah di run maka akan terbuat file baru berbentuk excel yang akan menampung hasil Text Prepocessing, berikut hasil file excel tersebut.","title":"Hasil"},{"location":"webmining/#vector-space-model","text":"Vector Space Model (VSM) adalah suatu model yang digunakan untuk mengukur kemiripan antara suatu dokumen dengan suatu query . Query dan dokumen dianggap sebagai vektor-vektor pada ruang n-dimensi, dimana t adalah jumlah dari seluruh term yang ada dalam leksikon. Leksikon adalah daftar semua term yang ada dalam indeks.","title":"Vector Space Model"},{"location":"webmining/#menggunakan-metode","text":"","title":"Menggunakan Metode"},{"location":"webmining/#bag-of-word","text":"Sebuah konsep yang diambil dari analisis teks, yaitu merepresentasikan dokumen sebagai sebuah kantung informasi-informasi penting tanpa mengurutkan setiap katanya. mainbasket_ambilfitur.py File mainbasket_ambilfitur.py ini berfungsi untuk mengambilseluruh kata pada semua dokumen. mainbasket_sastrawi_seringmuncul.py File mainbasket_sastrawi_seringmuncul.py ini digunakan untuk menghitung frekuensi kemunculan setiap kata dalam bentuk matrik kemudian hasil tersebut akan disimpan dalam bentuk file excel baru seperti pada gambar dibawah ini.","title":"Bag-of-Word"},{"location":"webmining/#tf-idf","text":"TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. Sedangkan IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. Rumus dari TF-IDF adalah TF x IDF. mainbasket_tfidf.py File mainbasket_tfidf.py ini berfungsi untuk melakukan proses perhitungan nilai setiap kata IDF terlebih dahulu, lalu akan melakukan perhitungan nilai TF-IDF dan menjadikan ke bentuk matrik VSM. for i in range(len(list_fitur)): temp=list_fitur[i] data[temp]*=w[i] print (data.head()) data.to_csv('baru_tfidf.csv') code tersebut adalah sebuah rumus untuk menghitung TF-IDF lalu hasil perhitungan tersebut diletakkan dalam file berformat CSV. Berikut adalah hasil dari file CSV.","title":"TF-IDF"},{"location":"webmining/#seleksi-fitur","text":"","title":"Seleksi Fitur"},{"location":"webmining/#definisi","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi.","title":"Definisi"},{"location":"webmining/#metode","text":"Dalam seleksi fitur kali ini, saya menggunakan beberapa metode , berikut adalah metode yang saya gunakan : Principal Component Analysis Model Based Ranking Univariate Feature Selection Langsuh saja kita bahas satu persatu","title":"Metode"},{"location":"webmining/#1-principal-component-analysis","text":"PCA (Principal Component Analysis) adalah teknik yang digunakan untuk menyederhanakan suatu data, dengan cara mentransformasi data secara linier sehingga terbentuk sistem koordinat baru dengan varians maksimum. Analisis komponen utama dapat digunakan untuk mereduksi dimensi suatu data tanpa mengurangi karakteristik data tersebut secara signifikan. Analisis komponen utama juga sering digunakan untuk menghindari masalah multikolinearitas antar peubah bebas dalam model regresi berganda.","title":"1. Principal Component Analysis"},{"location":"webmining/#2-model-based-ranking","text":"Model Based Ranking disini memungkinkan untuk memberi peringkat. Modl Based Ranking ini menggunakan prosedur Random Forest .","title":"2. Model Based Ranking"},{"location":"webmining/#random-forest","text":"Random forest (RF) adalah suatu algoritma yang digunakan pada klasifikasi data dalam jumlah yang besar. Klasifikasi random forest dilakukan melalui penggabungan pohon (tree) dengan melakukan training pada sampel data yang dimiliki. Penggunaan pohon (tree) yang semakin banyak akan mempengaruhi akurasi yang akan didapatkan menjadi lebih baik. Penentuan klasifikasi dengan random forest diambil berdasarkan hasil voting dari tree yang terbentuk. Pemenang dari tree yang terbentuk ditentukan dengan vote terbanyak. Pembangunan pohon (tree) pada random forest sampai dengan mencapai ukuran maksimum dari pohon data. def RandomForest(): from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score clf = RandomForestClassifier(n_estimators=2, max_depth=4) scores = [] num_features = len(X.columns) for i in range(num_features): col = X.columns[i] score = np.mean(cross_val_score(clf, X[col].values.reshape(-1, 1), y, cv=7)) scores.append((int(score * 100), col)) print (\">>>>>>>>>>>>>>>>>>>>\") print (i, \">\", col, \"score:\", score) Code tersebut berfungsi untuk melakukan proses perankingan semua fitur menggunakan Random Forest yang bertujuan untuk melihat fitur yang paling berpengaruh. scores = sorted(scores, reverse=True) print (\"Random Forest>>>>>>>\") print(\"The 5 best features selected by this method are :\") for i in range(5): print(scores[i][1]) print (\"The 5 worst features selected by this method are :\") for i in range(5): print(scores[len(scores) - 1 - i][1]) Code tersebut berfungsi untuk menampilkan 5 fitur dengan ranking terbaik dan 5 fitur dengan ranking terendah.","title":"Random Forest"},{"location":"webmining/#3-univariate-feature-selection","text":"Univariate Feature Selection disini untuk memilih fitur terbaik .Univariate Feature Selection ini menggunakan prosedur Chi Square.","title":"3. Univariate Feature Selection"},{"location":"webmining/#chi-square","text":"Prosedur Chi Square ini dengan mentabulasikan suatu variabel menjadi kategori dan menghitung statistik chi square. Uji kecocokan modelnya membandingkan observasi dan frekuensi harapan pada kategori untuk diuji tiap kategorinya. Uji Chi Square digunakan untuk menguji hubungan atau pengaruh dua buah variabel nominal dan mengukur kuatnya hubungan antar variabel. def UFS(cek, n_ranking): from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2, mutual_info_classif if cek == 1: print (\"UFS (Univariate Feature Selection) model chi^2 >>>>>>>\") test = SelectKBest(score_func=chi2, k=2) elif cek == 2: print (\"UFS (Univariate Feature Selection) mutual_info_classif >>>>>>>\") test = SelectKBest(score_func=mutual_info_classif, k=2) test.fit(X, y) scores = [] for i in range(num_features): score = test.scores_[i] scores.append((score, X.columns[i])) print (sorted(scores, reverse=True)) print_best_worst(scores, n_ranking) Code tersebut digunakan untuk menjalankan proses prosedur chi square .","title":"Chi Square"},{"location":"webmining/#clustering","text":"","title":"Clustering"},{"location":"webmining/#definisi_1","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Metode Clustering sendiri ada banyak.Kali ini yang saya gunakan adalah K-Means Clustering dan Shilhouette Coefisient.","title":"Definisi"},{"location":"webmining/#k-means","text":"K-means merupakan salah satu algoritma clustering. Tujuan algoritma ini yaitu untuk membagi data menjadi beberapa kelompok. Algoritma ini menerima masukan berupa data tanpa label kelas. Hal ini berbeda dengan supervised learning yang menerima masukan berupa vektor (\u00adx\u00ad1 , y1) , (\u00adx\u00ad2 , y2) , \u2026, (\u00adx\u00adi , yi), di mana xi merupakan data dari suatu data pelatihan dan yi merupakan label kelas untuk xi. Algoritma ini akan mengelompokkan data atau objek ke dalam k buah kelompok tersebut. Pada setiap cluster terdapat titik pusat (centroid) yang merepresentasikan cluster tersebut. Algoritma K-Means : Tentukan jumlah klaster yang ingin dibentuk dan tetapkan pusat cluster k. Menggunakan jarak euclidean kemudian hitung setiap data ke pusat cluster. Kelompokkan data ke dalam cluster dengan jarak yang paling pendek dengan persamaan. Hitung pusat cluster yang baru menggunakan persamaan. Ulangi langkah dua sampai dengan empat sehingga sudah tidak ada lagi data yang berpindah ke kluster yang lain. python clusterer = KMeans(n_clusters=n_cluster) preds = clusterer.fit_predict((df)) centers = clusterer.cluster_centers_ Code diatas menyatakan banyaknya cluster.","title":"K-means"},{"location":"webmining/#shilhouette-coefisient","text":"Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. Tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a i Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b i . Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien :","title":"Shilhouette Coefisient"},{"location":"webmining/#clustering-k-mean-dengan-total-corpus-total-fitur-asli","text":"banyak_cluster = list(range(2, 150)) for n_cluster in banyak_cluster: clusterer = KMeans(n_clusters=n_cluster) preds = clusterer.fit_predict((df)) centers = clusterer.cluster_centers_ score = silhouette_score(df, preds, metric='euclidean') temp.append(score) temp_pred.append(preds) # print (\"Untuk kluster={},silhoute score :{} \".format(n_cluster, repr(score))) # print(preds) print (\"kluster terbaik\") print (\"kluster ke > \" + str(temp.index(max(temp)) + 2) + \" >silhout> \" + str(max(temp))) Code tersebut berfungsi untuk melakukan proses clustering menggunakan K-mean clustering dengan menggunakan euclidean serta menghitung nilai silhoutte pada setiap cluster dengan menggunakan total corpus asli, untuk selanjutnya menampilkan nilai silhoutte terbaik pada cluster tertentu.","title":"Clustering K-mean dengan total corpus (total fitur) asli."},{"location":"webmining/#clustering-k-mean-feature-selection-random-forest","text":"def RFE(cek, n_ranking): from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression if cek == 1: print (\"RFE (Recursive Feature Elimination) model LogisticRegression >>>>>>>\") rfe = RFE(LogisticRegression(), n_features_to_select=1) elif cek == 2: print (\"RFE (Recursive Feature Elimination) model RandomForest >>>>>>>\") rfe = RFE(RandomForestClassifier(n_estimators=10), n_features_to_select=1) rfe.fit(X, y) scores = [] for i in range(num_features): scores.append((rfe.ranking_[i], X.columns[i])) print (sorted(scores, reverse=True)) print_best_worst(scores, n_ranking) Code tersebut berfungsi untuk melakukan proses seleksi fitur menggunakan metode Random Forest menggunakan n_estimator pohon keputusan. def UFS(cek, n_ranking): from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2, mutual_info_classif if cek == 1: print (\"UFS (Univariate Feature Selection) model chi^2 >>>>>>>\") test = SelectKBest(score_func=chi2, k=2) elif cek == 2: print (\"UFS (Univariate Feature Selection) mutual_info_classif >>>>>>>\") test = SelectKBest(score_func=mutual_info_classif, k=2) test.fit(X, y) scores = [] for i in range(num_features): score = test.scores_[i] scores.append((score, X.columns[i])) print (sorted(scores, reverse=True)) print_best_worst(scores, n_ranking) Code diatas digunakan untuk melakukan proses seleksi fitur menggunakan metode Chi Square .","title":"Clustering (K-mean) + Feature Selection (Random Forest)."},{"location":"webmining/#evaluasi","text":"Dalam kasus ini data yang digunakan oleh penulis berasal dari website \"https://www.mainbasket.com/c/4/berita/abl\" dengan page sebanyak 5 page yang dimana setiap page terdapat 7 berita, total keseluruhan ada 35 data , yang terdiri dari judul serta isi berita.","title":"Evaluasi"},{"location":"webmining/#hasil_1","text":"","title":"Hasil"},{"location":"webmining/#seleksi-fitur-random-forest-dengan-k-mean","text":"Pada percobaan ini, penulis menngunakan seleksi fitur random forest namun dengan nilai n yang berbeda. Perlu diketahui bahwa n menunjukkan banyaknya pohon keputusan yang terdapat pada random forest. Percobaan n_estimator Banyak Fitur yang digunakan Silhoutte terbaik n_Cluster ke 1 5 100 0.5403243593327233 2 ke 2 5 50 0.2160825606285164 5 ke 3 50 100 0.516949295993263 2 ke 4 50 50 0.24651372752710796 3","title":"Seleksi Fitur Random Forest dengan K-mean"},{"location":"webmining/#hasil-percobaan-terbaik","text":"","title":"Hasil Percobaan Terbaik"},{"location":"webmining/#seleksi-fitur-chi-square-dengan-k-mean","text":"Pada percobaan ini, penulis menngunakan seleksi fitur chi square namun dengan menggunakan banyak fitur yang digunakan secara berbeda. Percobaan Banyak Fitur yang digunakan Silhoutte terbaik n_Cluster ke 1 35 0.6134604631137835 2 ke 2 100 0.5327485060311361 2 Hasil Percobaan ke 1 Hasil Percobaan ke 2","title":"Seleksi Fitur Chi Square dengan K-mean"},{"location":"webmining/#kesimpulan","text":"Dari beberapa percobaan diatas, dapat menyimpulkan bahwa metode clustering K-mean dengan seleksi fitur \"Random Forest\" menggunakan jumlah fitur sebanyak 100 fitur serta menggunkan n_estimator=5 adalah metode terbaik sejauh ini. Refrensi : https://yudiagusta.wordpress.com/clustering/ https://id.wikipedia.org/wiki/K-means http://depandienda.it.student.pens.ac.id/file/kmeans_references.pdf https://www.researchgate.net/publication/323365687_Algoritme_Genetika_Untuk_Optimasi_K-Means_Clustering_Dalam_Pengelompokan_Data_Tsunami https://repository.ipb.ac.id/handle/123456789/14020 https://id.wikipedia.org/wiki/Analisis_komponen_utama https://id.wikipedia.org/wiki/Random_forest http://datariset.com/olahdata/detail/olah-data-jogja-uji-chi-square https://www.kaggle.com/dkim1992/feature-selection-ranking https://www.datariset.com/olahdata/detail/olah-data-jogja-uji-chi-square https://www.rumusstatistik.com/2015/03/analisis-komponen-utama-principal.html https://liyantanto.wordpress.com/2011/06/28/pencarian-dengan-metode-vektor-space-model-vsm/ https://repository.widyatama.ac.id/xmlui/bitstream/handle/123456789/7151/Daftar%20Istilah.pdf?sequence=8 https://informatikalogi.com/term-weighting-tf-idf/ https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&ved=2ahUKEwjS4bfq6_rhAhW763MBHdFRCvgQFjAEegQIARAC&url=http%3A%2F%2Fmalifauzi.lecture.ub.ac.id%2Ffiles%2F2016%2F02%2FText-Pre-Processing-v2.pptx&usg=AOvVaw3lqK3cdfxwBPYWPH1omJ-w https://id.wikipedia.org/wiki/Tokenisasi https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/ https://devtrik.com/python/text-preprocessing-dengan-python-nltk/ https://informatikalogi.com/text-preprocessing/ https://www.kompasiana.com/jalaludin.ax/54f67c7fa333112b758b4ebf/membuat-web-crawling http://prowebscraping.com/web-scraping-vs-web-crawling/ https://beritati.blogspot.com/2015/06/sekilas-tentang-web-mining-penambangan.html","title":"Kesimpulan"},{"location":"webstructure/","text":"Web Structure Mining \u00b6 Definisi \u00b6 Web struncture mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Tujuan \u00b6 Tujuan penambangan struktur Web adalah untuk menghasilkan ringkasan struktural tentang situs Web dan halaman Web. Secara teknis, penambangan konten Web terutama berfokus pada struktur dokumen dalam, sementara penambangan struktur Web menemukan struktur tautan hyperlink di tingkat antar-dokumen. Penambangan struktur Web akan mengkategorikan halaman web dan menghasilkan informasi, seperti kesamaan dan hubungan antara berbagai situs web. Penambangan struktur web juga dapat memiliki arah lain - menemukan struktur dokumen Web itu sendiri. Jenis penambangan struktur ini dapat digunakan untuk mengungkapkan struktur (skema) halaman Web, ini akan baik untuk tujuan navigasi dan memungkinkan untuk membandingkan / mengintegrasikan skema halaman Web. Tools \u00b6 [ ] Python 2.7 / bisa menggunakan spyder [ ] Library : BeautifulSoup4, requests, pa, networks, matplotlib [ ] Website target: Mainbasket Start Mining \u00b6 Pertama yang harus lakukan adalah mencrawling data. Cara pertama adalah mengimport library terlebih dahulu import requests from bs4 import BeautifulSoup Selanjutnya yang perlu dilakukan adalah membuat fungsi untuk mendapatkan semua tautan dan menggambar nodges dan edge. def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: print(\"Error 404 : Page \"+src+\" not found\") return list() def add_nodes_from_list(old_nodes, new_nodes): for node in new_nodes: if not node in old_nodes: old_nodes.append(node) return old_nodes def add_edges_from_list(old_edges, from_, to_list): for to_ in to_list: edge = (from_, to) if not edge in old_edges: old_edges.append(edge) return old_edges Code di atas akan menyelusuri semua tag dan mendapatkan tag 'http' di situs web lalu menggambar semua simpul dan tepi untuk setiap tautan lain yang terkait dengan situs web tersebut. Setelah itu mendefinisikan situs web, lalu membuat struktur dan memanggil fungsi di atas. root = \"https://www.mainbasket.com/\" nodelist = [root] done = [root] edgelist = [] from datetime import datetime start_time = datetime.now() deep1 = getAllLinks(root) print(\"banyak root : \", len(deep1)) c=1 for link in deep1: edge = (root, link) if not edge in edgelist: edgelist.append(edge) if not link in done: nodelist.append(link) done.append(link) deep2 = getAllLinks(link) for link2 in deep2: edge = (link, link2) if not edge in edgelist: edgelist.append(edge) if not link2 in done: nodelist.append(link2) done.append(link2) deep3 = getAllLinks(link2) for link3 in deep3: edge = (link2, link3) if not edge in edgelist: edgelist.append(edge) if not link3 in nodelist: nodelist.append(link3) Code diatas digunakn unuk menemukan struktur kedalaman 3. code ini juga akan menghitung dan meanmpilakn root serta menampilakn setaip tautan tidak aktif di situs web tersebut. Page Rank \u00b6 Definisi \u00b6 Pagerank adalah algoritma dari mesin telusur Google yang berfungsi menganalisis link dan memberikan nomor atau peringkat ke setiap halaman web yang ada di internet. Peringkat pagerank sebuah halaman web dihitung dari 0-10, semakin besar pagerank yang dimiliki sebuah halaman web maka semakin besar peluang untuk mendapatkan posisi paling atas pada hasil penelusuran Google. Fungsi \u00b6 Fungsi utama dari pagerank yaitu menganalisis berbagai link yang masuk (backlink) kemudian akan dihitung berapa jumlah link yang masuk (Inbound) dan link yang keluar (Outbound) dari sebuah halaman web tersebut, yang kemudian akan menghasilkan pagerank yang kita dapatkan. Pagerank juga berfungsi menentukan situs web mana yang lebih penting atau populer mulai dari skala yang paling tinggi 10 hingga skala yang paling rendah yaitu 0. Semakin tinggi pagerank yang dimiliki sebuah halaman web maka semakin populer halaman web tersebut, karena dengan pagerank yang tinggi yang milikinya itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lain. Menghitung Page Rank \u00b6 damping = 0.85 max_iterr = 100 error_toleransi = 0.01 pr = nx.pagerank(g, alpha - damping, max_iter1=max_iterr, tol1=error_toleransi) Code diatas merupakan perhitungan pagerank dengan library yang terdapat pada python yaitu networkx. Langkah selanjutnya adala menampung hasil perhitungan kedalam list print(\"keterangan node:\") nodelist = g.nodes label= {} data=[] for i, key in enumerate(nodelist): data.append((pr[key], key)) label[key]=i Selanjutnya nilai pagerank di urutkan urut = data.copy() for x in range(len(urut)): for y in range(len(urut)): if urut[x][0] > urut[y][0]: urut[x], urut[y] = urut[y], urut[x] urut = pd.DataFrame(data, None, (\"Pagerank\", \"Node\")) Graph \u00b6 dan langkah terakhir adalah membuat graph g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) plt.title('Link Graph') nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() Refrensi: http://www.cyberartsweb.org/cpace/ht/lanman/wsm1.htm> https://www.caramanual.com/2017/01/apa-itu-pagerank-inilah-pengertian-dan.html","title":"Web Structure Mining"},{"location":"webstructure/#web-structure-mining","text":"","title":"Web Structure Mining"},{"location":"webstructure/#definisi","text":"Web struncture mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web.","title":"Definisi"},{"location":"webstructure/#tujuan","text":"Tujuan penambangan struktur Web adalah untuk menghasilkan ringkasan struktural tentang situs Web dan halaman Web. Secara teknis, penambangan konten Web terutama berfokus pada struktur dokumen dalam, sementara penambangan struktur Web menemukan struktur tautan hyperlink di tingkat antar-dokumen. Penambangan struktur Web akan mengkategorikan halaman web dan menghasilkan informasi, seperti kesamaan dan hubungan antara berbagai situs web. Penambangan struktur web juga dapat memiliki arah lain - menemukan struktur dokumen Web itu sendiri. Jenis penambangan struktur ini dapat digunakan untuk mengungkapkan struktur (skema) halaman Web, ini akan baik untuk tujuan navigasi dan memungkinkan untuk membandingkan / mengintegrasikan skema halaman Web.","title":"Tujuan"},{"location":"webstructure/#tools","text":"[ ] Python 2.7 / bisa menggunakan spyder [ ] Library : BeautifulSoup4, requests, pa, networks, matplotlib [ ] Website target: Mainbasket","title":"Tools"},{"location":"webstructure/#start-mining","text":"Pertama yang harus lakukan adalah mencrawling data. Cara pertama adalah mengimport library terlebih dahulu import requests from bs4 import BeautifulSoup Selanjutnya yang perlu dilakukan adalah membuat fungsi untuk mendapatkan semua tautan dan menggambar nodges dan edge. def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: print(\"Error 404 : Page \"+src+\" not found\") return list() def add_nodes_from_list(old_nodes, new_nodes): for node in new_nodes: if not node in old_nodes: old_nodes.append(node) return old_nodes def add_edges_from_list(old_edges, from_, to_list): for to_ in to_list: edge = (from_, to) if not edge in old_edges: old_edges.append(edge) return old_edges Code di atas akan menyelusuri semua tag dan mendapatkan tag 'http' di situs web lalu menggambar semua simpul dan tepi untuk setiap tautan lain yang terkait dengan situs web tersebut. Setelah itu mendefinisikan situs web, lalu membuat struktur dan memanggil fungsi di atas. root = \"https://www.mainbasket.com/\" nodelist = [root] done = [root] edgelist = [] from datetime import datetime start_time = datetime.now() deep1 = getAllLinks(root) print(\"banyak root : \", len(deep1)) c=1 for link in deep1: edge = (root, link) if not edge in edgelist: edgelist.append(edge) if not link in done: nodelist.append(link) done.append(link) deep2 = getAllLinks(link) for link2 in deep2: edge = (link, link2) if not edge in edgelist: edgelist.append(edge) if not link2 in done: nodelist.append(link2) done.append(link2) deep3 = getAllLinks(link2) for link3 in deep3: edge = (link2, link3) if not edge in edgelist: edgelist.append(edge) if not link3 in nodelist: nodelist.append(link3) Code diatas digunakn unuk menemukan struktur kedalaman 3. code ini juga akan menghitung dan meanmpilakn root serta menampilakn setaip tautan tidak aktif di situs web tersebut.","title":"Start Mining"},{"location":"webstructure/#page-rank","text":"","title":"Page Rank"},{"location":"webstructure/#definisi_1","text":"Pagerank adalah algoritma dari mesin telusur Google yang berfungsi menganalisis link dan memberikan nomor atau peringkat ke setiap halaman web yang ada di internet. Peringkat pagerank sebuah halaman web dihitung dari 0-10, semakin besar pagerank yang dimiliki sebuah halaman web maka semakin besar peluang untuk mendapatkan posisi paling atas pada hasil penelusuran Google.","title":"Definisi"},{"location":"webstructure/#fungsi","text":"Fungsi utama dari pagerank yaitu menganalisis berbagai link yang masuk (backlink) kemudian akan dihitung berapa jumlah link yang masuk (Inbound) dan link yang keluar (Outbound) dari sebuah halaman web tersebut, yang kemudian akan menghasilkan pagerank yang kita dapatkan. Pagerank juga berfungsi menentukan situs web mana yang lebih penting atau populer mulai dari skala yang paling tinggi 10 hingga skala yang paling rendah yaitu 0. Semakin tinggi pagerank yang dimiliki sebuah halaman web maka semakin populer halaman web tersebut, karena dengan pagerank yang tinggi yang milikinya itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lain.","title":"Fungsi"},{"location":"webstructure/#menghitung-page-rank","text":"damping = 0.85 max_iterr = 100 error_toleransi = 0.01 pr = nx.pagerank(g, alpha - damping, max_iter1=max_iterr, tol1=error_toleransi) Code diatas merupakan perhitungan pagerank dengan library yang terdapat pada python yaitu networkx. Langkah selanjutnya adala menampung hasil perhitungan kedalam list print(\"keterangan node:\") nodelist = g.nodes label= {} data=[] for i, key in enumerate(nodelist): data.append((pr[key], key)) label[key]=i Selanjutnya nilai pagerank di urutkan urut = data.copy() for x in range(len(urut)): for y in range(len(urut)): if urut[x][0] > urut[y][0]: urut[x], urut[y] = urut[y], urut[x] urut = pd.DataFrame(data, None, (\"Pagerank\", \"Node\"))","title":"Menghitung Page Rank"},{"location":"webstructure/#graph","text":"dan langkah terakhir adalah membuat graph g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) plt.title('Link Graph') nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() Refrensi: http://www.cyberartsweb.org/cpace/ht/lanman/wsm1.htm> https://www.caramanual.com/2017/01/apa-itu-pagerank-inilah-pengertian-dan.html","title":"Graph"}]}